{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     1
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 4:  Matrix rank, low-rank approximation, SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap of the previous lecture\n",
    "\n",
    "- Unitary matrices preserve the norm\n",
    "- There are two \"basic\" classes of unitary matrices, Householder and Givens.\n",
    "- Every unitary matrix can be represented as a product of those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Todays lecture\n",
    "- Matrix rank\n",
    "- Skeleton decomposition\n",
    "- Low-rank approximation\n",
    "- Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix and linear spaces\n",
    "A matrix can be considered as a sequence of vectors, its columns:\n",
    "$$\n",
    "   A = [a_1, \\ldots, a_m], \n",
    "$$\n",
    "where $a_m \\in \\mathbb{C}^n$.  \n",
    "A matrix-by-vector product is equivalent to taking a linear combination of those columns  \n",
    "$$\n",
    "   y =  Ax, \\quad \\Longleftrightarrow y = a_1 x_1 + a_2 x_2 + \\ldots +a_m x_m.\n",
    "$$\n",
    "\n",
    "This is a special case of **block matrix notation** that we have already seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear dependence\n",
    "\n",
    "Of the key points is the concept of **linear dependence** between vectors.\n",
    "\n",
    "The vectors are called linearly dependent, if there exist non-zero coefficients $x_i$ such that  \n",
    "$$\\sum_i a_i x_i = 0,$$\n",
    "or in the matrix form,\n",
    "$$\n",
    "   Ax = 0, \\quad \\Vert x \\Vert \\ne 0.\n",
    "$$\n",
    "In this case, we say that the matrix $A$ has a non-trivial **nullspace** or **kernel**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear space\n",
    "A **linear space** spanned by vectors $a_1, \\ldots, a_m$ is defined as all possible vectors of the form \n",
    "$$\n",
    "   \\mathcal{L}(a_1, \\ldots, a_m) = \\{y: y = \\sum_{i=1}^m a_i x_i \\}, \n",
    "$$\n",
    "where $x_i$ are some coefficients and $a_i, i = 1, \\ldots, m$ are given vectors. \n",
    "\n",
    "In the matrix form, the linear space is a collection of the vectors of the form  \n",
    "$$y = A x.$$\n",
    "This set is also called the **range** of the matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel and image\n",
    "\n",
    "We will use notation $\\text{im}(A)$ for the image of $A$ (i.e., the set of vectors $y = Ax$ for some $x$) and \n",
    "\n",
    "$\\text{ker}(A)$ for the kernel of $A$ (i.e., the set of vectors $x$ for which $Ax = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimension of a linear space\n",
    "\n",
    "The dimension of a linear space is defined as the minimal number $m$ such that there exists a matrix $A$ with $m$ columns and for each $y \\in \\mathcal{L}$ there exists a vector $x$ such that\n",
    "\n",
    "$$y = A x.$$\n",
    "\n",
    "The dimension of a linear space has a direct connection to the matrix rank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix rank\n",
    "What is a matrix rank?  \n",
    "\n",
    "A matrix rank is defined as the maximal number of linearly independent columns in a matrix, \n",
    "\n",
    "or the **dimension of its column space**.  \n",
    "\n",
    "You can also use linear combination of rows to define the rank.  \n",
    "\n",
    "I.e., formally there are two ranks: column rank and row rank of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "** Theorem**  \n",
    "The dimension of the column space of the matrix is equal to the dimension of the row space of the matrix.\n",
    "\n",
    "Proof?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Full-rank matrix\n",
    "\n",
    "A matrix $A \\in \\mathbb{R}^{m \\times n}$ is called **full-rank**, if $\\mathrm{rank}(A) = \\min(m, n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Small-dimensional subspaces\n",
    "\n",
    "Suppose, we have a linear subspace, spanned by $n$ vectors. \n",
    "\n",
    "Let these vector be random with elements from $N(0, 1)$\n",
    "\n",
    "What is the probability of the fact that this subspace has dimension $m < n$?\n",
    "\n",
    "\n",
    "Another formulation: a random matrix has full rank with probability 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random subspaces are \"lower-dimensional\"\n",
    "\n",
    "A very interesting result is the \n",
    "\n",
    "**Johnsonâ€“Lindenstrauss lemma** \n",
    "\n",
    "Given $0 < \\epsilon < 1$, a set of points in $\\mathbb{R}^N$ and $n > \\frac{8 \\log m}{\\epsilon^2}$\n",
    "\n",
    "and linear map $f$ from $\\mathbb{R}^N \\rightarrow \\mathbb{R}^M$\n",
    "\n",
    "$$(1 - \\epsilon) \\Vert u - v \\Vert^2 \\leq \\Vert f(u) - f(v) \\Vert^2 \\leq (1 + \\epsilon) \\Vert u - v \\Vert^2.$$\n",
    "\n",
    "It is not very practical due to the dependence from $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton decomposition\n",
    "A very useful representation for computation of the matrix rank is the **skeleton decomposition**, which can be graphically represented as follows:  \n",
    "<img src=\"cross-pic.png\">\n",
    "or in the matrix form\n",
    "$$\n",
    "   A = U \\widehat{A}^{-1} V^{\\top},\n",
    "$$\n",
    "where $U$ are some $r$ columns of $A$, $V^{\\top}$ are some rows of $A$, $\\widehat{A}$ is the submatrix on the intersection, which should be **nonsingular**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remark\n",
    "\n",
    "We have not yet formally defined the inverse, so just a reminder:\n",
    "\n",
    "An inverse of the matrix $P$ the matrix $Q = P^{-1}$ such that  \n",
    "$ P Q = QP = I$.  \n",
    "If the matrix is square and has full rank (i.e. equal to $n$) then the inverse exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proof for the skeleton decomposition\n",
    "Let $U$ be the $r$ columns based on the submatrix $\\widehat{A}$. Then they are linearly independent. Take any other column $u$ of $A$. Then it is a linear combination of the columns of $U$, i.e.  \n",
    "$u = U x$.  \n",
    "These are $n$ equations. We take $r$ of those corresponding to the rows that contain $\\widehat{A}$ and get the equation  \n",
    "$$\\widehat{u} = \\widehat{A} x,$$  therefore  \n",
    "$$x = \\widehat{A}^{-1} \\widehat{u},$$ and that gives us the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A closer look\n",
    "Any rank-$r$ matrix can be written in the form\n",
    "$$A = U \\widehat{A}^{-1} V^{\\top},$$\n",
    "where $U$ is $n \\times r$, $V$ is $m \\times r$ and $\\widehat{A}$ is $r \\times r$, or \n",
    "$$\n",
    "   A = U' V'^{\\top}.\n",
    "$$\n",
    "So, every rank-$r$ matrix can be written as a product of a \"tall\" matrix $U'$ by a long matrix $V'$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the index form, it is  \n",
    "$$\n",
    "   a_{ij} = \\sum_{\\alpha=1}^r u_{i \\alpha} v_{j \\alpha}.\n",
    "$$\n",
    "For rank 1 we have\n",
    "$$\n",
    "   a_{ij} = u_i v_j,\n",
    "$$\n",
    "i.e. it is a separation of indices and rank-$r$ is a sum of rank-$1$ matrices!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storage\n",
    "It is interesting to note, that for the rank-$r$ matrix \n",
    "$$A = U V^{\\top}$$\n",
    "only $U$ and $V$ can be stored, which gives us $(n+m) r$ parameters, so it can be used for compression. We can also compute matrix-by-vector product much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#A fast matrix-by-vector product demo\n",
    "import numpy as np\n",
    "n = 1000\n",
    "r = 10\n",
    "u = np.random.randn(n, r)\n",
    "v = np.random.randn(n, r)\n",
    "a = u.dot(v.T)\n",
    "x = np.random.randn(n)\n",
    "%timeit a.dot(x)\n",
    "%timeit u.dot(v.T.dot(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing matrix rank\n",
    "We can also try to compute the matrix rank using the built-in ```np.linalg.matrix_rank``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Computing matrix rank\n",
    "import numpy as np\n",
    "n = 50 \n",
    "a = np.ones((n, n))b\n",
    "print 'Rank of the matrix:', np.linalg.matrix_rank(a)\n",
    "b = a + 1e-5 * np.random.randn(n, n)\n",
    "print 'Rank of the matrix:', np.linalg.matrix_rank(b, tol=1e-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Instability of the matrix rank\n",
    "For any rank-$r$ matrix $A$ with $r < \\min(m, n)$ there is a matrix $B$ such that its rank rank is equal to $\\min(m, n)$ and\n",
    "$$\n",
    " \\Vert A - B \\Vert = \\varepsilon.\n",
    "$$\n",
    "So, does this mean that numerically matrix rank has no meaning? (I.e., small perturbations lead to full rank)!  \n",
    "The solution is to compute the best **rank-r** approximation to a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular value decomposition\n",
    "To compute low-rank approximation, we need to compute **singular value decomposition**.\n",
    "\n",
    "**Theorem** Any matrix $A\\in \\mathbb{C}^{n\\times m}$ can be written as a product of three matrices:  \n",
    "$$\n",
    "   A = U \\Sigma V^*,\n",
    "$$\n",
    "where $U$ is an $n \\times K$ unitary matrix, $V$ is an $m \\times K$ unitary matrix, $K = \\min(m, n)$, <br> $\\Sigma$ is a diagonal matrix with non-negative elements $\\sigma_1 \\geq  \\ldots, \\geq \\sigma_K$ on the diagonal. <br>\n",
    "Moreover, if $\\text{rank}(A) = r$, then $\\sigma_{r+1} = \\dots = \\sigma_K = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proof\n",
    "* Matrix is $A^*A$ Hermitian, hence diagonalizable in unitary basis (will be discussed further in the course).\n",
    "* $A^*A>0$ (positive definite), so eigenvalues are positive.\n",
    "Therefore, there exists unitary $V = [v_1, \\dots, v_n]$ such that\n",
    "\n",
    "$$\n",
    "    V^* A^* A V = \\text{diag}(\\sigma_1^2,\\dots, \\sigma_n^2), \\quad \\sigma_1\\geq \\sigma_2\\geq \\dots \\geq \\sigma_n.\n",
    "$$\n",
    "\n",
    "Let $\\sigma_i = 0$ for $i>r$, where $r$ is some integer. <br>\n",
    "Let $V_r= [v_1, \\dots, v_r]$, $\\Sigma_r = \\text{diag}(\\sigma_1, \\dots,\\sigma_r)$. Hence\n",
    "\n",
    "$$\n",
    "    V^*_r A^* A V_r = \\Sigma_r^2 \\quad \\Longrightarrow \\quad (\\Sigma_r^{-1} V_r^* A^*) (A V_r\\Sigma_r^{-1} ) = I.\n",
    "$$\n",
    "\n",
    "As a result matrix $U_r = A V_r\\Sigma_r^{-1}$ satisfies $U_r^* U_r = I$ and hence has orthogonal columns. <br>\n",
    "Adding any orthogonal columns that are orthogonal to columns in $U_r$ and denoting this matrix as $U$ we arrive at\n",
    "\n",
    "$$\n",
    "    AV = U \\begin{bmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix}\\quad \\Longrightarrow \\quad U^* A V = \\begin{bmatrix}\\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Since multiplication by non-singular matrices does not change rank of $A$, we have $r = \\text{rank}(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Corollary 1**: $A = \\displaystyle{\\sum_{\\alpha=1}^r} \\sigma_\\alpha u_\\alpha v_\\alpha^*$ or elementwise $a_{ij} = \\displaystyle{\\sum_{\\alpha=1}^r} \\sigma_\\alpha u_{i\\alpha} \\overline{v}_{\\alpha i}$\n",
    "\n",
    "**Corollary 2**: $$\\text{ker}(A) = \\text{span}\\{v_{r+1},\\dots,v_n\\}$$\n",
    "$$\\text{im}(A) = \\text{span}\\{u_{1},\\dots,u_r\\}$$\n",
    "$$\\text{ker}(A^*) = \\text{span}\\{u_{r+1},\\dots,u_n\\}$$\n",
    "$$\\text{im}(A^*) = \\text{span}\\{v_{1},\\dots,v_r\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Eckart-Young theorem\n",
    "\n",
    "The best low-rank approximation can be computed by SVD.\n",
    "\n",
    "**Theorem:** Let $r < \\text{rank}(A)$, $A_r = U_r \\Sigma_r V_r^*$. Then\n",
    "\n",
    "$$\n",
    "    \\min_{\\text{rank}(B)=r} = \\|A - B\\|_2 = \\|A - A_r\\|_2 = \\sigma_{r+1}.\n",
    "$$\n",
    "\n",
    "The same holds for $\\|\\cdot\\|_F$ with $\\|A - A_r\\|_F = \\sqrt{\\sigma_{r+1}^2 + \\dots + \\sigma_{\\min (n,m)}^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proof  \n",
    "Since $\\text{rank} (B) = r$, it holds $\\text{dim}~\\text{ker}~B = n-r$. \n",
    "\n",
    "\n",
    "Hence there exists $z\\not=0$ such that $z\\in \\text{ker}(B) \\cap \\text{span}(v_1,\\dots,v_{r+1})$ (as $\\text{dim}\\{v_1,\\dots,v_{r+1}\\} = r+1$).\n",
    "\n",
    "Fix $\\|z\\| = 1$. Therefore,\n",
    "$$\n",
    "    \\|A-B\\|_2^2 \\geq \\|(A-B)z\\|_2^2 = \\|Az\\|_2^2 = \\sum_{i=1}^{n} \\sigma_i (v_i^*z)^2 =\\sum_{i=1}^{r+1} \\sigma_i (v_i^*z)^2 \\geq \\sigma_{r+1}^2.\\quad \\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main result on low-rank approximation\n",
    "\n",
    "Computation of the best rank-$r$ approximation is equivalent to setting $\\sigma_{r+1}= 0, \\ldots, \\sigma_K = 0$. The error is then determined by the omitted singular value!  \n",
    "$$\n",
    "   \\min_{A_r} \\Vert A - A_r \\Vert = \\sigma_{r+1},\n",
    "$$\n",
    "that is why it is important to look at the decay of the singular values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing SVD\n",
    "\n",
    "Algorithms for the computation of the SVD are tricky.\n",
    "\n",
    "But for numerics, we can use numpy already!\n",
    "\n",
    "Let us go back to the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Computing matrix rank\n",
    "import numpy as np\n",
    "n = 50 \n",
    "a = np.ones((n, n))\n",
    "print 'Rank of the matrix:', np.linalg.matrix_rank(a)\n",
    "b = a + 1e-5 * np.random.randn(n, n)\n",
    "print 'Rank of the matrix:', np.linalg.matrix_rank(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "u, s, v = np.linalg.svd(b)\n",
    "print s[1]/s[0]\n",
    "r = 1\n",
    "u1 = u[:, :r]\n",
    "s1 = s[:r]\n",
    "v1 = v[:r, :]\n",
    "a1 = u1.dot(np.diag(s1).dot(v1))\n",
    "print np.linalg.norm(b - a1, 2)/s[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Separation of variables in functions\n",
    "\n",
    "We can use SVD to compute approximations of **function-related** matrices, i.e. the matrices of the form  \n",
    "$$a_{ij} = f(x_i, y_j),$$\n",
    "where $f$ is a certain function, and $x_i, \\quad i = 1, \\ldots, n$ and $y_j, \\quad j = 1, \\ldots, m$ are some **one-dimensional grids**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "n = 1000\n",
    "a = [[1.0/(i+j+1) for i in xrange(n)] for j in xrange(n)] #Hilbert matrix \n",
    "a = np.array(a)\n",
    "u, s, v = np.linalg.svd(a)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.semilogy(s[:30]/s[0])\n",
    "#We have very good low-rank approximation of it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can do something more interesting, like function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 128\n",
    "t = np.linspace(0, 5, n)\n",
    "x, y = np.meshgrid(t, t)\n",
    "f = 1.0 / (x + y + 0.5)\n",
    "u, s, v = np.linalg.svd(f, full_matrices=False)\n",
    "r = 10\n",
    "u = u[:, :r]\n",
    "s = s[:r]\n",
    "v = v[:r, :] #Mind the transpose here!\n",
    "fappr = u.dot(np.diag(s).dot(v))\n",
    "er = np.linalg.norm(fappr - f, 'fro') / np.linalg.norm(f, 'fro')\n",
    "print er\n",
    "plt.semilogy(s/s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And do some 3D plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(x, y, f)\n",
    "ax.set_title('Original function')\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.plot_surface(x, y, fappr - f)\n",
    "ax.set_title('Approximation error with rank=%d, err=%3.1e' % (r, er))\n",
    "fig.subplots_adjust()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear factor analysis & low-rank\n",
    "Consider a linear factor model, \n",
    "\n",
    "$$y = Ax, $$ where $y$ is a vector of length $n$, and $x$ is a vector of length $r$.  \n",
    "The data is organizes as samples: we observe vectors  \n",
    "$$y_1, \\ldots, y_T,$$\n",
    "then the factor model can be written as  \n",
    "$$\n",
    "  Y = AX,\n",
    "$$\n",
    "where $Y$ is $n \\times T$, $A$ is $n \\times r$ and $X$ is $r \\times T$. This is exactly a rank-$r$ model: it tells us that the vector lie in a small subspace!  \n",
    "We also can use SVD to recover this subspace (but not the independent components). Principal component analysis can be done by SVD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular values of a random Gaussian matrix\n",
    "\n",
    "What is the singular value decay of a random matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n = 1000\n",
    "a = np.random.randn(n, n)\n",
    "u, s, v = np.linalg.svd(a)\n",
    "plt.semilogy(s/s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take home message\n",
    "- Matrix rank definition\n",
    "- Skeleton approximation and dyadic representation of a rank-$r$ matrix\n",
    "- Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- Linear systems\n",
    "- Inverse matrix\n",
    "- Condition number\n",
    "- Linear least squares\n",
    "- Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- Think of course projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "\n",
       "</style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
